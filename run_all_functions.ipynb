{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import config, convert2yolov1\n",
    "from src.util import video2images, get_video_folder_paths, seleccionar_imagen, merge_datasets\n",
    "#from src.model import autolabel_images, predict_and_visualice\n",
    "from src.dataset_processing import run_processing_dataset\n",
    "import os\n",
    "import json\n",
    "import supervision as sv\n",
    "import cv2\n",
    "from src.data_augmentation import augment_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline autolabeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autolabeling\n",
    "from tqdm import tqdm\n",
    "\n",
    "video_paths = get_video_folder_paths(config.VIDEO_DIR_PATH)\n",
    "for video_path in tqdm(video_paths):\n",
    "    autolabeling.mypipeline(\n",
    "        video_path=video_path,\n",
    "        image_dir_path=config.IMAGE_DIR_PATH,\n",
    "        frame_rate=80,\n",
    "        ontology=json.load(open(os.path.join(video_paths[0], \"ontology.json\"))),\n",
    "        output_images=config.DATASET_DIR_PATH,\n",
    "        extension=\".jpg\",\n",
    "        box_threshold =0.35,\n",
    "        text_threshold = 0.25,\n",
    "        num_datasets=1,\n",
    "        height = None,\n",
    "        width = None,\n",
    "        use_yolo = True,\n",
    "        not_val=True,\n",
    "        model_path = \"trained_models/yolov8m_640x480_cf_9cam_v34.pt\",\n",
    "        confidence=0.4,\n",
    "        iou=0.7,\n",
    "        imgsz=640,\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: CUDA not available. GroundingDINO will run very slowly.\n",
      "trying to load grounding dino directly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Alexander\\Go2Future\\Autolabel_roboflow\\.venv\\Lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3484.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Labeling ./images/Train_linea_de_caja_640x480\\camera65_640x480_20240311_105646-00000.jpg for dataset 1:   0%|          | 0/4 [00:00<?, ?it/s]d:\\Alexander\\Go2Future\\Autolabel_roboflow\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:881: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "d:\\Alexander\\Go2Future\\Autolabel_roboflow\\.venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "Labeling ./images/Train_linea_de_caja_640x480\\camera65_640x480_20240311_105646-00003.jpg for dataset 1: 100%|██████████| 4/4 [02:03<00:00, 30.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled dataset 1 created - ready for distillation.\n"
     ]
    }
   ],
   "source": [
    "# Si solo tienes una carpeta de imágenes\n",
    "from src.model import autolabel_images\n",
    "autolabel_images(\n",
    "            input_folder=\"./images/Train_linea_de_caja_640x480\",\n",
    "            ontology=json.load(open(os.path.join(\"./images/Train_linea_de_caja_640x480\", \"ontology.json\"))),\n",
    "            box_threshold=0.35,\n",
    "            text_threshold=0.25,\n",
    "            output_folder=\"./dataset\",\n",
    "            extension=\".jpg\",\n",
    "            num_datasets=1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run pipeline autotracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autotracking\n",
    "from tqdm import tqdm\n",
    "from src.dataset_processing import run_processing_dataset\n",
    "\n",
    "video_paths = get_video_folder_paths(config.VIDEO_DIR_PATH)\n",
    "for video_path in tqdm(video_paths):\n",
    "            autotracking.mypipeline_track(\n",
    "                video_path,\n",
    "                image_dir_path=config.IMAGE_DIR_PATH,\n",
    "                frame_rate=40,\n",
    "                output_images=config.DATASET_DIR_PATH,\n",
    "                extension=\".jpg\",\n",
    "                height = None,\n",
    "                width = None,\n",
    "                model_path = \"trained_models/yolov8m_640x480_cf_9cam_v34.pt\",\n",
    "                confidence=0.4,\n",
    "                iou=0.7,\n",
    "                imgsz=640,\n",
    "\n",
    "            )\n",
    "# Procesar Merged_Dataset/train para eliminar errores de anotación\n",
    "contenido = os.listdir(config.DATASET_DIR_PATH)\n",
    "print(\"Contenido \\n\", contenido)\n",
    "\n",
    "for data_p in contenido:\n",
    "        output_path = os.path.join(config.DATASET_DIR_PATH, data_p)\n",
    "        run_processing_dataset(\n",
    "            os.path.join(output_path, \"train\", \"images\"),\n",
    "            os.path.join(output_path, \"train\", \"labels\"),\n",
    "            max_size=0.8,\n",
    "            min_size=0.1,\n",
    "            iou_threshold=0.7,\n",
    "            remove_empty=False,\n",
    "            remove_large=False,\n",
    "            remove_small=False,\n",
    "            remove_overlapping=False,\n",
    "            remove_multiple=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacer merged de datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Merging datasets...\n",
      "100%|██████████| 9/9 [00:00<00:00, 35.57it/s]\n",
      "INFO:root:Merged datasets created at d:\\Alexander\\Go2Future\\Autolabel_roboflow\\dataset\\Merged_Dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'person', 'person', 'person', 'person', 'person', 'person', 'person', 'person']\n",
      "['person']\n"
     ]
    }
   ],
   "source": [
    "# Unir datasets\n",
    "# crear el path a la carpeta de merged dataset dentro de dataset\n",
    "output_path = os.path.join(config.DATASET_DIR_PATH, \"Merged_Dataset\")\n",
    "    # obtener el nombre de cada dataset individual dentro de dataset\n",
    "folders = os.listdir(config.DATASET_DIR_PATH)\n",
    "    # Lista de paths de cada dataset individual\n",
    "dataset_paths = [os.path.join(config.DATASET_DIR_PATH, folder) for folder in folders]\n",
    "    # Hacer el merge\n",
    "merge_datasets(dataset_paths, output_path, val=False) # val = False para autotracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remover etiquetas incorrectas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed empty\n",
      "removed empty\n"
     ]
    }
   ],
   "source": [
    "# Procesar Merged_Dataset/train para eliminar errores de anotación\n",
    "output_path = os.path.join(config.DATASET_DIR_PATH, \"Merged_Dataset\")\n",
    "run_processing_dataset(\n",
    "        os.path.join(output_path, \"train\", \"images\"),\n",
    "        os.path.join(output_path, \"train\", \"labels\"),\n",
    "        min_size=0.1,\n",
    "        max_size=0.8,\n",
    "        iou_threshold=0.7,\n",
    "        remove_empty= True,\n",
    "        remove_large= False,\n",
    "        remove_small= False,\n",
    "        remove_overlapping= False,\n",
    "        remove_multiple= False,\n",
    "    )\n",
    "# Procesar Merged_Dataset/valid para eliminar errores de anotación\n",
    "run_processing_dataset(\n",
    "        os.path.join(output_path, \"valid\", \"images\"),\n",
    "        os.path.join(output_path, \"valid\", \"labels\"),\n",
    "        min_size=0.1,\n",
    "        max_size=0.8,\n",
    "        iou_threshold=0.7,\n",
    "        remove_empty= True,\n",
    "        remove_large= False,\n",
    "        remove_small= False,\n",
    "        remove_overlapping= False,\n",
    "        remove_multiple= False,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacer data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path=os.path.join(config.DATASET_DIR_PATH, \"cam52\")\n",
    "augmented_dataset_path=os.path.join(config.DATASET_DIR_PATH, \"Augmented_Dataset\")\n",
    "# creating aumented datased directory\n",
    "if not os.path.exists(augmented_dataset_path):\n",
    "    os.makedirs(augmented_dataset_path)\n",
    "\n",
    "augment_dataset(dataset_path, augmented_dataset_path, just_rezize=False, augmented_for=5, height=480, width=640)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convertir a formato YOLOv1 de CVAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir un solo dataset dentro de DATASET_DIR_PATH\n",
    "dataset_path=os.path.join(config.DATASET_DIR_PATH, \"Train_linea_de_caja_640x480_1\")\n",
    "convert2yolov1.convert_to_yolov1_format(dataset_path, with_val=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir todos los datasets dentro de un directorio a formato YOLOv1\n",
    "# Directorio que contiene todos los datasets\n",
    "dataset_dir_path = config.DATASET_DIR_PATH\n",
    "\n",
    "# Iterar a través de los subdirectorios en DATASET_DIR_PATH\n",
    "for dataset_name in os.listdir(dataset_dir_path):\n",
    "    dataset_path = os.path.join(dataset_dir_path, dataset_name)\n",
    "    \n",
    "    # Verificar si el elemento en DATASET_DIR_PATH es un directorio\n",
    "    if os.path.isdir(dataset_path):\n",
    "        print(f\"Procesando el dataset: {dataset_name}\")\n",
    "        convert2yolov1.convert_to_yolov1_format(dataset_path, with_val=False)\n",
    "\n",
    "print(\"Proceso de conversión completado para todos los datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convertir anotaciones de Yolov1 CVAT a Yolov8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear un directorio dentro de config.DATASET_DIR_PATH\n",
    "yolo8_out =os.path.join(config.DATASET_DIR_PATH,\"CF_9cam_with_val_640x480_corrected\")\n",
    "os.makedirs(yolo8_out, exist_ok=True)\n",
    "\n",
    "convert2yolov1.convert_to_yolov8_format(os.path.join(config.DATASET_DIR_PATH,\"CF_9cam_with_val_640x480_corrected_YOLOV1.zip\"),yolo8_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizar anotaciones en formato YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presiona 'q' para salir.\n"
     ]
    }
   ],
   "source": [
    "import visualizar_anotaciones\n",
    "\n",
    "# Permitir al usuario seleccionar el directorio\n",
    "directorio_seleccionado = visualizar_anotaciones.seleccionar_directorio()\n",
    "if not directorio_seleccionado:\n",
    "        print(\"No se ha seleccionado un directorio. Saliendo.\")\n",
    "        exit()\n",
    "\n",
    "    # Obtener los paths de las carpetas \"images\" y \"labels\"\n",
    "directorio_imagenes = os.path.join(directorio_seleccionado, \"images\")\n",
    "directorio_labels = os.path.join(directorio_seleccionado, \"labels\")\n",
    "\n",
    "# Verificar la existencia de las carpetas \"images\" y \"labels\"\n",
    "if not os.path.exists(directorio_imagenes) or not os.path.exists(directorio_labels):\n",
    "        print(\n",
    "            \"No se encontraron las carpetas 'images' y 'labels'. Asegúrate de que la estructura sea correcta.\"\n",
    "        )\n",
    "        exit()\n",
    "\n",
    "# Ejecutar la función con los directorios proporcionados\n",
    "visualizar_anotaciones.mostrar_imagenes_y_anotaciones(directorio_imagenes, directorio_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesar dataset de ReID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/Pilar_11cam_ReID\\annotations.xml\n"
     ]
    }
   ],
   "source": [
    "# Crear Formato Market1501 a partir de CVAT for images 1.1\n",
    "from src import crear_market1501\n",
    "import os\n",
    "\n",
    "xml_dir = \"./dataset/Pilar_11cam_ReID\"  # \"D:\\Alexander\\Go2Future\\Autolabel_roboflow\\dataset\\cvat_3cam_3id\"\n",
    "xml_file = \"annotations.xml\"\n",
    "xml_path = os.path.join(xml_dir, xml_file)\n",
    "output_dir = \"default\"\n",
    "print(xml_path)\n",
    "\n",
    "# obtener el nombre de la carpeta de xml_dir\n",
    "output_folder = os.path.basename(xml_dir)\n",
    "\n",
    "if output_dir == \"default\":\n",
    "    output_dir = os.path.join(xml_dir, f\"{output_folder}_market_format\")\n",
    "else:\n",
    "    output_dir = os.path.join(output_dir, f\"{output_folder}_market_format\")\n",
    "\n",
    "cvat_annotations = crear_market1501.parse_cvat_annotations(xml_path)\n",
    "crear_market1501.process_cvat_annotations(\n",
    "    cvat_annotations,\n",
    "    output_dir,\n",
    "    path_to_images=os.path.join(xml_dir, \"images\", \"train\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conformar las carpetas train, test y query\n",
    "from src import split_train_test_query\n",
    "\n",
    "dataset_path = \"./dataset/Pilar_11cam_ReID/Pilar_11cam_ReID_market_format\"  # Reemplazar con la ruta de tu dataset\n",
    "output_path = \"./dataset/Pilar_11cam_ReID\"  # Reemplazar con la ruta deseada para el nuevo dataset\n",
    "\n",
    "split_train_test_query.split_dataset(dataset_path, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
